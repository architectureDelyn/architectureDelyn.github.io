---
layout: default
modal-id: 1
date: 2014-07-18
img: cabin.png
title: "1. 데이터 품질 거버넌스 구축"
alt: image-alt
description: "[배경]<br>- 데이터 아키텍처 사상에서 품질관리는 크게 구조/표준/활용/값 관점으로 나눌 수 있음<br>- 기존까지 사내에 데이터 아키텍처/표준화/모델링/품질관리 원칙 등이 부재한 상황<br>- 데이터 모델 검토, 데이터 표준화, 매핑정의서 관리 등의 업무는 수행하고 있었으나, 데이터 값 관점의 품질관리(dq) 업무는 전무한 상황<br><br><br>[해결 완료]<br>1. 기존 업무를 분석하여 데이터 아키텍처/표준화/모델링 원칙 제정<br> - 개요/용어/목적/역할/세부내용 등으로 구성 되었으며, 각 원칙별 내용은 아래와 같음<br> - 데이터 표준화 원칙 : 표준 구성(단어/용어/도메인/인포타입 등) 정의 및 관계, 표준별 세부 규칙, 명명규칙 등<br> - 데이터 모델링 원칙 : 테이블 명명규칙, pre/postfix 구분, 식별자 설계 원칙, 정규화/반정규화, 이력관리, 슈퍼-서브타입 변환 등<br><br>2. 데이터 품질관 현황 파악 및 레퍼런스 조사<br> - 데이터 품질 관련 수행/미수행 중인 task 정의<br> - 상용 dq 솔루션, 오픈소스(GX, Ydata 등) 등 카카오뱅크 환경을 고려한 도입/개발 검토<br> - Python을 활용하여 PoC 대상 DB 선정 및 데이터 프로파일링(ad-hoc) 진행<br> - 프로파일링 결과를 통해 Issue-up 진행(현황, 문제점, 개선 방향 등)<br> - Data Mesh 환경에 적합한 사용자 중심 데이터 품질관리 시스템 MVP 개발 및 설계(ERD, 시퀀스 다이어그램, 시스템 구조도 등)<br><br>3. 데이터 카탈로그 수집기 개발<br> - dq 업무(데이터 프로파일링, 품질 진단) 수행을 위해선 최신의 카탈로그 정보를 수집/유지해야 함<br> - 따라서 Oracle, MySQL, PostgreSQL DB 데이터 소스에 대한 카탈로그를 수집하여 Soft Delete 및 Upsert 할 수 있도록 하는 프로그램 개발(Django 기반)<br><br>4. 데이터 프로파일러 개발<br> - 프로파일링 수집 항목 정의(정적/패턴)<br> - Oracle, MySQL, PostgreSQL DB 데이터 소스에 대해 최고/최저/카디널리티/패턴 등 항목 수집이 가능한 프로파일러 개발(Django 기반)<br><br>5. 데이터 품질관리 시스템 개발<br> - 데이터 카탈로그 수집, 데이터 프로파일링 task 및 스케줄 관리, 수집제외 스키마 관리 등 Admin 개발<br> - 수집된 카탈로그 및 프로파일링 정보와 함께 메타데이터를 한 눈에 조회/검색할 수 있는 웹서비스 개발(데이터 디스커버리 목적)<br><br><br>[해결 진행중]<br>1. 데이터 품질 원칙 제정<br> - 개발된 품질관리 시스템 및 Cloud-Native, MSA 등을 모두 고려한 원칙 제정<br><br>2. 데이터 품질관리 시스템 고도화<br> - 별도의 검증 DB 및 정보계를 활용하지 않고 서비스 DB를 직접 품질진단 할 수 있는 아키텍처 설계 및 개발<br> - task(카탈로그 수집, 프로파일링, 품질진단 등) 관리 방식을 Celery에서 Airflow+K8S 방식으로 전환<br> - Hive metastore, Snowflake, Redshift 등 빅데이터 환경의 데이터 소스에 대해서도 관리할 수 있도록 개발 및 영역 확대 등"


---
